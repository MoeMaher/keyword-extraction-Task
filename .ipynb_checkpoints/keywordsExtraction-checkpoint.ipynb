{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import nltk.data\n",
    "import pattern3\n",
    "import re\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import brown\n",
    "word_list = brown.words()\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and visualize the data to gain insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv('documents-fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4087, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[\"core_std\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7599104\n"
     ]
    }
   ],
   "source": [
    "# Concatinate all words from all the documents.\n",
    "allDocuments = ''\n",
    "for i in range(len(docs)):\n",
    "    if(not isinstance(docs['content'][i], type(0.0))):\n",
    "        allDocuments = allDocuments + str(docs['content'][i])\n",
    "print(len(allDocuments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(['m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating pipeline functions for handling the data (content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions that handle the text format and style of writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the tokens to lowercase/uppercase\n",
    "def convert_letters(tokens, style = \"lower\"):\n",
    "    if (styl# converts the tokens to lowercase/uppercase\n",
    "def convert_letters(tokens, style = \"lower\"):\n",
    "    if (style == \"lower\"):\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    else :\n",
    "e == \"lower\"):\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    else :\n",
    "        tokens = [token.upper() for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# remove blancs from text \n",
    "def remove_blanc(tokens):\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# expand contractions ex. this's -> this is\n",
    "def expand_contractions(sentence, contraction_mapping=CONTRACTION_MAP): \n",
    "     \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),  \n",
    "                                      flags=re.IGNORECASE|re.DOTALL) \n",
    "    def expand_match(contraction): \n",
    "        match = contraction.group(0) \n",
    "        first_char = match[0] \n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                        \n",
    "        expanded_contraction = first_char+expanded_contraction[1:] \n",
    "        return expanded_contraction \n",
    "         \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \n",
    "    return expanded_sentence \n",
    "\n",
    "# convert the text into unicode\n",
    "def remove_accent(tokens):\n",
    "    tokens = [unidecode.unidecode(token) for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# remove the stopwords from the tokenized text \n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(wordlist, stopwords=stopword_list):\n",
    "    return [w for w in wordlist if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions that handle the spelling mistakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Probability of `word`.\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    return WORDS[word] / N\n",
    "\n",
    "# Most probable spelling correction for word relative to the corpus in WORDS.\n",
    "def correct(word): \n",
    "    if len(word) > 1:\n",
    "        return max(candidates(word), key=P)\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# if there is another word in the documents similar to the input world with a relativily high\n",
    "# occurance in the documnets return the it\n",
    "# else return the input word \n",
    "def properify(word): \n",
    "    mostProbable = max(candidates_weird(word), key=P)\n",
    "    if(known([word]) and P(mostProbable)-P(word) > (P(word)/2) and len(word) > 2):\n",
    "        return mostProbable\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Generate possible spelling corrections for word\n",
    "def candidates(word): \n",
    "    return  known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "# Generate possible spelling similar to the word\n",
    "def candidates_weird(word): \n",
    "    return known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "# The subset of `words` that appear in the dictionary of WORDS\n",
    "def known(words): \n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "# All edits that are one edit away from `word`.\n",
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# All edits that are two edits away from `word`.\n",
    "def edits2(word): \n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "# impractical dont use it\n",
    "# All edits that are three edits away from `word`.\n",
    "def edits3(word): \n",
    "    return (e3 for e1 in edits1(word) for e3 in edits2(e1))\n",
    "\n",
    "# take a list of tokens and call correct on each token\n",
    "def correct_from_tokens(tokens):\n",
    "    WORDS = Counter(words(' '.join(word_list)))\n",
    "    return [correct(w) for w in tokens]\n",
    "    \n",
    "# take a list of tokens and call properify on each token\n",
    "def remove_weird_from_tokens(tokens):\n",
    "    WORDS = Counter(words(allDocuments))\n",
    "    return [properify(w) for w in tokens]\n",
    "    \n",
    "# ** not used **\n",
    "def text_blob_clean(tokens):\n",
    "    cleanBlob = TextBlob(' '.join(tokens))\n",
    "    return cleanBlob.correct()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the trained module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the classifier ...\n",
      "Loaded model from disk\n",
      "compiling the classifier\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# the location of the classifier\n",
    "path_to_classifier = '/home/maher/keywordExtraction/keywords-extraction-Project/RNN/savedModel_Python'\n",
    "\n",
    "# predict method take an input the document tokenized and output a list of predictions the \n",
    "# rest of the parameters are self explanatory\n",
    "def predict(inWords, bag_of_words, most_common, classifier, label_encoder, onehot_encoder, accuracy):\n",
    "\n",
    "    inWords = [w for w in inWords if w in bag_of_words]\n",
    "\n",
    "    test_integer_encoded = label_encoder.transform(inWords)\n",
    "    test_integer_encoded = test_integer_encoded.reshape(len(test_integer_encoded), 1)\n",
    "    X = onehot_encoder.transform(test_integer_encoded)\n",
    "    X = np.array([[w] for w in X])\n",
    "\n",
    "    pred = classifier.predict(X)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    simi_out = [inWords[i] for i in range(len(inWords)) if pred[i] > accuracy]\n",
    "    stemmed_out = []\n",
    "    out = []\n",
    "    for word in simi_out:\n",
    "        if stemmer.stem(word) not in stemmed_out:\n",
    "            out.append(word)\n",
    "            stemmed_out.append(stemmer.stem(word))\n",
    "\n",
    "    return np.unique(out)\n",
    "\n",
    "\n",
    "\n",
    "print('loading the classifier ...')\n",
    "# load json and create model\n",
    "# TODO: change the path \n",
    "json_file = open(path_to_classifier+'/classifier.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(path_to_classifier + \"/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "print('compiling the classifier')\n",
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer= 'RMSprop',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# loading LabelEncoder, OneHot Encoder, stopwords and the bag of words used \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load(path_to_classifier + '/label_encoder.npy')\n",
    "\n",
    "with open(path_to_classifier + \"/onehot_encoder.txt\", \"rb\") as fp:   # Unpickling\n",
    "    onehot_encoder = pickle.load(fp)\n",
    "with open(path_to_classifier + \"/common_words.txt\", \"rb\") as fp:   # Unpickling\n",
    "    common_words = pickle.load(fp)\n",
    "with open(path_to_classifier + \"/bag_of_words.txt\", \"rb\") as fp:   # Unpickling\n",
    "    bag_of_words = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# method getTags take as an input the document tokenized and the accuracy needed and return the predictions  \n",
    "def getTags(tokens, accuracy):\n",
    "    tags = predict(tokens, bag_of_words, common_words, loaded_model, label_encoder, onehot_encoder, accuracy)\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tags of a specific document, by its number\n",
    "def getTagsByDocNum(documentNumber):\n",
    "\n",
    "    inputDoc = docs['content'][documentNumber]\n",
    "    inputDocList = regexp_tokenize(expand_contractions(inputDoc), pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "    no_weird_words = remove_weird_from_tokens(remove_blanc(convert_letters(inputDocList)))\n",
    "\n",
    "    clean = correct_from_tokens(no_weird_words)\n",
    "    # moreClean = text_blob_clean(clean)\n",
    "\n",
    "    tags = getTags(clean, 0.7)\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4087\n",
      "113\n",
      "0\n",
      "197\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# get the Tags of All the documnets in a list, each document separated by '###'\n",
    "def getAllTags():\n",
    "    numberOfDocuments = 2 \n",
    "    allPredictions = ''\n",
    "    print(len(docs))\n",
    "    for documentNumber in range(numberOfDocuments):\n",
    "        if(not isinstance(docs['content'][documentNumber], type(0.0))):\n",
    "            inputDoc = docs['content'][documentNumber]\n",
    "            inputDocList = regexp_tokenize(expand_contractions(inputDoc), pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "            no_weird_words = remove_weird_from_tokens(remove_blanc(convert_letters(inputDocList)))\n",
    "\n",
    "            clean = correct_from_tokens(no_weird_words)\n",
    "\n",
    "            tags = getTags(clean, 0.7)\n",
    "\n",
    "            allPredictions = allPredictions + ' '.join(tags)\n",
    "            allPredictions = allPredictions + ' ### '\n",
    "\n",
    "            print(len(allPredictions))\n",
    "            print(documentNumber)\n",
    "    return allPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions100 = '''builder cards children curve frequency guided inventory mm random resource ride sheet state straight team word ### cards children curve frequency guided inventory mate music random resource sheet word ### builder cards children curve frequency guided inventory motor random resource sheet word ### cards children color cut frequency guided inventory mate notch random resource sheet word ### builder cards children curve frequency guided inventory random resource school sheet straight word ### arms builder cards children curve frequency guided inventory random resource sheet strip word ### builder cards children curve frequency guided house inventory music random resource sheet word ### arc builder cards children frame group inventory power repeat resource sentence sheet stories straight word ### builder cards children feedback frequency guided random repeat resource sentence sprung stories straight word ### builder cards children feedback frequency guided inventory random repeat resource stories word ### builder cards children curve feedback frequency game guided inventory prostate random repeat resource sentence stories straight word ### builder cards children curve feedback frequency gray guided mate push random repeat resource sentence stories straight word ### builder cards children desk feedback frequency guided random repeat resource sentence stories straight word ### builder cards children feedback frequency guided inventory random repeat resource sentence stories word ### builder cards children curve feedback frequency guided heart house inventory mate random repeat resource rough sentence stories straight word ### builder capital cards children feedback finger frequency guided house inventory random repeat resource sentence stories word ### builder cards children curve edge feedback frequency random repeat resource sentence stories straight word ### builder cards children curve feedback frequency guided inventory random repeat resource sentence straight tract word ### builder cards children curve feedback frequency guided inventory mate powder random repeat resource stories straight word zebra ### builder cards children curve feedback frequency guided inventory random repeat resource sentence stories straight word ### @ group management resource word ### @ group management resource word ### @ group management resource word ### labels resource word ### labels resource word ### labels resource word ### children media random repeat ride word ### builder cards children error graph learning repeat word ### builder cards children error graph learning repeat word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### arc children frequency group house job repeat sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### children sentence trees word ### banks cards children feedback frame modeling professional sentence speech word ### banks cards children feedback frame modeling professional sentence speech word ### banks cards children feedback frame modeling professional sentence speech word ### children knowledge stories tree word ### children desk distance guided music rules selection sentence web word ### children desk distance guided music rules selection sentence web word ### children group guided language mm path repeat sentence transparency word ### capital cards children classroom guided labels rates self sentence transparency word ### builder cards children frequency repeat word ### builder cards children frequency repeat word ### builder cards children frequency repeat word ### children group mate resource sentence ### children group mate resource sentence ### children group mate resource sentence ### children group mate resource sentence ### children expert guided information personal riding ### children direct information personal word ### children direct information personal word ### children frequency group mark repeat word ### children frequency group mark repeat word ### @ answering children information knowledge learning mark mate memories mm repeat selection sentence social stories transparency word ### @ answering children information knowledge learning mark mate memories mm repeat selection sentence social stories transparency word ### children frame repeat team word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### children distance group sheet word ### children distance group sheet word ### children group guided language sentence ### cards children classroom cut health labels resource school sheet word ### cards children classroom cut health labels resource school sheet word ### builder cards children dual frame patterns repeat sentence transparency word ### builder cards children dual frame patterns repeat sentence transparency word ### children coding frequency spacing torso white word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### bus children paired personal repeat selection ### bus children paired personal repeat selection ### cards children connections phone word ### cards children connections phone word ### cards children connections phone word ### '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 294), ('children', 92), ('word', 86), ('cards', 50), ('repeat', 47), ('sentence', 46), ('guided', 42), ('resource', 42), ('frequency', 39), ('group', 34), ('builder', 25), ('random', 24), ('inventory', 16), ('stories', 15), ('feedback', 15), ('transparency', 15), ('curve', 13), ('sheet', 12), ('straight', 12), ('speech', 12), ('mate', 11), ('selection', 11), ('frame', 10), ('knowledge', 10), ('bank', 10), ('@', 8), ('professional', 8), ('mark', 8), ('house', 7), ('state', 6), ('labels', 6), ('ride', 5), ('rates', 5), ('information', 5), ('personal', 5), ('mm', 4), ('music', 4), ('notch', 4), ('learning', 4), ('distance', 4), ('head', 4), ('logging', 4), ('cut', 3), ('school', 3), ('desk', 3), ('management', 3), ('nest', 3), ('banks', 3), ('modeling', 3), ('classroom', 3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predTokens = nltk.tokenize.word_tokenize(predictions100)\n",
    "fdist  = FreqDist(predTokens)\n",
    "most30 = fdist.most_common(50)\n",
    "print(most30)\n",
    "most30 = [w for (w,x) in most30]\n",
    "most30.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mm', 'ride', 'team', '#', '#']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedPredictions100 = [w for w in predTokens if w not in most30]\n",
    "cleanedPredictions100[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builder cards children curve frequency guided inventory mm random resource ride sheet state straight team word # # # cards children curve frequency guided inventory mate music random resource sheet word # # # builder cards children curve frequency guided inventory motor random resource sheet word # # # cards\n",
      ".............................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mm ride team ',\n",
       " 'music ',\n",
       " 'motor ',\n",
       " 'color cut notch ',\n",
       " 'school ',\n",
       " 'arms strip ',\n",
       " 'music ',\n",
       " 'arc power ',\n",
       " 'sprung ',\n",
       " '',\n",
       " 'game prostate ',\n",
       " 'gray push ',\n",
       " 'desk ',\n",
       " '',\n",
       " 'heart rough ',\n",
       " 'capital finger ',\n",
       " 'edge ',\n",
       " 'tract ',\n",
       " 'powder zebra ',\n",
       " '',\n",
       " 'management ',\n",
       " 'management ',\n",
       " 'management ',\n",
       " 'labels ',\n",
       " 'labels ',\n",
       " 'labels ',\n",
       " 'media ride ',\n",
       " 'error graph learning ',\n",
       " 'error graph learning ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'arc job ',\n",
       " 'nest notch ride ',\n",
       " 'nest notch ride ',\n",
       " 'nest notch ride ',\n",
       " 'trees ',\n",
       " 'banks modeling ',\n",
       " 'banks modeling ',\n",
       " 'banks modeling ',\n",
       " 'tree ',\n",
       " 'desk distance music rules web ',\n",
       " 'desk distance music rules web ',\n",
       " 'language mm path ',\n",
       " 'capital classroom labels rates self ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'expert information personal riding ',\n",
       " 'direct information personal ',\n",
       " 'direct information personal ',\n",
       " '',\n",
       " '',\n",
       " 'answering information learning memories mm social ',\n",
       " 'answering information learning memories mm social ',\n",
       " 'team ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'distance ',\n",
       " 'distance ',\n",
       " 'language ',\n",
       " 'classroom cut health labels school ',\n",
       " 'classroom cut health labels school ',\n",
       " 'dual patterns ',\n",
       " 'dual patterns ',\n",
       " 'coding spacing torso white ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'head ',\n",
       " 'head ',\n",
       " 'head ',\n",
       " 'head ',\n",
       " 'logging rates ',\n",
       " 'logging rates ',\n",
       " 'logging rates ',\n",
       " 'logging rates ',\n",
       " 'bus paired personal ',\n",
       " 'bus paired personal ',\n",
       " 'connections phone ',\n",
       " 'connections phone ',\n",
       " 'connections phone # # #']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(' '.join(predTokens[:50]))\n",
    "print('.............................')\n",
    "FinalPredictions100 = (' '.join(cleanedPredictions100)).split('# # # ')\n",
    "FinalPredictions100[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
