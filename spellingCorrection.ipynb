{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.probability import FreqDist\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional, Dropout,Embedding\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflowjs as tfjs\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import webtext\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_classifier = '/home/maher/Desktop/optimaKeywordsTask/newClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownCorpus = ''\n",
    "for category in brown.categories(): \n",
    "    words = brown.words(categories=category)\n",
    "    text = \" \".join(words)\n",
    "    brownCorpus = brownCorpus + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6127059\n",
      "991\n"
     ]
    }
   ],
   "source": [
    "print(len(brownCorpus))\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(len(sent_tokenize_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffledList = []\n",
    "for i in range(5):\n",
    "    random.shuffle(sent_tokenize_list)\n",
    "    shuffledList = shuffledList + sent_tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"\"] * len(lines)\n",
    "Y_train = [0] * len(lines)\n",
    "title = [0] * len(lines) \n",
    "\n",
    "\n",
    "for i in range(len(lines)) :\n",
    "\tl = nltk.word_tokenize(lines[i])\n",
    "\tstrings[i] = l[0]\n",
    "\ttitle[i] = l[1]\n",
    "\tY_train[i] = l[2]\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "Y_train_main = [int(label) for label in Y_train]\n",
    "X_train_main = strings\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# collecting the 100 most common words to discard it when predicting\n",
    "print('analizing words ...')\n",
    "freqd = FreqDist(X_train_main)\n",
    "common_words = [ w[0] for w in freqd.most_common(100)]\n",
    "with open(\"common_words.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(common_words, fp)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "print('processing the base words ...')\n",
    "x_train = X_train_main\n",
    "y_train = Y_train_main\n",
    "\n",
    "y_train = [y_train[i] for i in range(len(y_train)) if x_train[i] in model.vocab]\n",
    "x_train = [word for word in x_train if word in model.vocab] # array of words\n",
    "# x_train = list(nltk.bigrams(x_train))\n",
    "y_train = y_train[:len(x_train)]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "\n",
    "# this is used in developing envirnoment while testing the hyper parameters\n",
    "\n",
    "# y_test = [ y_test[i] for i in range(len(y_test)) if x_test[i] in model.vocab and x_test[i] in x_train ]\n",
    "# x_test = [ word for word in x_test if word in model.vocab and word in x_train] # array of words\n",
    "# # x_test = list(nltk.bigrams(x_test))\n",
    "# y_test = y_test[:len(x_test)]\n",
    "\n",
    "bag_of_words = createBagOfWords(x_train);\n",
    "\n",
    "print('encoding the words ...')\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(x_train)\n",
    "integer_encoded = indexEncoder(x_train, bag_of_words)\n",
    "\n",
    "print(integer_encoded[0:10])\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "X_train = onehot_encoder.fit_transform(integer_encoded)\n",
    "X_train = np.array([[w] for w in X_train])\n",
    "\n",
    "# test_integer_encoded = label_encoder.transform(x_test)\n",
    "# test_integer_encoded = test_integer_encoded.reshape(len(test_integer_encoded), 1)\n",
    "# X_test = onehot_encoder.transform(test_integer_encoded)\n",
    "# X_test = np.array([[w] for w in X_test])\n",
    "\n",
    "Y_train = np.array(y_train) # answers to vectors\n",
    "# X_train = np.array([[np.append(model[word],freqd.freq(word))] for word in x_train]) # array of vectors\n",
    "\n",
    "# Y_test = np.array(y_test) # answers to vectors\n",
    "# X_test = np.array([[np.append(model[word],freqd.freq(word))] for word in x_test]) # array of vectors\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# print('saving x_train ..')\n",
    "# with open(\"x_train.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(x_train, fp)\n",
    "# print('saving y_train ..')\n",
    "# with open(\"y_train.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(y_train, fp)\n",
    "# print('saving x_test ..')\n",
    "# with open(\"x_test.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(x_test, fp)\n",
    "# print('saving y_tr ..')\n",
    "# with open(\"y_test.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(y_test, fp)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "print('intializing the classifier ...')\n",
    "RNNClassifier = Sequential()\n",
    "RNNClassifier.add(LSTM(124,  input_shape = (None, len(X_train[0][0]))))\n",
    "RNNClassifier.add(Dense(64, activation = 'sigmoid'))\n",
    "RNNClassifier.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "RNNClassifier.compile(loss='binary_crossentropy',\n",
    "                     optimizer='rmsprop',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "print('fitting the classifier ...')\n",
    "history = RNNClassifier.fit(X_train, Y_train, batch_size=300, epochs=20)\n",
    "        #   callbacks=[TestCallback((X_test, Y_test), x_train, common_words )])\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# save the model for python uses! #########\n",
    "\n",
    "# print(RNNClassifier.evaluate(X_test, Y_test, verbose=0))\n",
    "        # print('\\\\nTesting loss: {}, acc: {}\\\\n'.format(loss, acc))\n",
    "\n",
    "model_json = RNNClassifier.to_json()\n",
    "with open(\"CNNClassifier.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "serialize weights to HDF5\n",
    "RNNClassifier.save_weights(\"model.h5\") # TODO: check right path\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# save the model for Javascript uses! ######\n",
    " \n",
    "# tfjs.converters.save_keras_model(RNNClassifier, path_to_save_classifier)\n",
    "\n",
    "#############################################################################\n",
    "    \n",
    "# save the tools needed for further predictions.\n",
    "\n",
    "thefile = open(\"common_words.txt\", 'w')\n",
    "for item in common_words:\n",
    "  thefile.write(\"%s\\n\" % item)\n",
    "\n",
    "thefile = open(\"bag_of_words.txt\", 'w')\n",
    "for item in bag_of_words:\n",
    "  thefile.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a call back function that print the accuracy and the confusion matrix by the end of each epoch\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data, y_test, common_words):\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "        self.common_words = common_words\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        \n",
    "        pred = (self.model.predict(x),0).flatten()\n",
    "        print(pred[0])\n",
    "        for i in range(len(pred)) :\n",
    "            if(pred[i] > 0.7 and self.y_test[i] not in self.common_words):\n",
    "                pred[i] = 1\n",
    "            else :\n",
    "                pred[i] = 0\n",
    "        print(pred[0])\n",
    "        print(confusion_matrix(y.flatten(),pred ))\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\\\nTesting loss: {}, acc: {}\\\\n'.format(loss, acc))\n",
    "\n",
    "# returns the indices of the words and the strange words will be -1\n",
    "def indexEncoder(strs, bagOfWords):\n",
    "    out = []\n",
    "    for word in strs:\n",
    "        if word in bagOfWords:\n",
    "            out.append([bagOfWords.index(word)])\n",
    "        else:\n",
    "            out.append([-1])\n",
    "    return out\n",
    "    \n",
    "def createBagOfWords(words):\n",
    "    return list(set(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
