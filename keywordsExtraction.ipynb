{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import nltk.data\n",
    "import pattern3\n",
    "import re\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import brown\n",
    "word_list = brown.words()\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and visualize the data to gain insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv('documents-fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4087, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[\"core_std\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7599104\n"
     ]
    }
   ],
   "source": [
    "# Concatinate all words from all the documents.\n",
    "allDocuments = ''\n",
    "for i in range(len(docs)):\n",
    "    if(not isinstance(docs['content'][i], type(0.0))):\n",
    "        allDocuments = allDocuments + str(docs['content'][i])\n",
    "print(len(allDocuments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(['m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating pipeline functions for handling the data (content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions that handle the text format and style of writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the tokens to lowercase/uppercase\n",
    "def convert_letters(tokens, style = \"lower\"):\n",
    "    if (style == \"lower\"):\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    else :\n",
    "        tokens = [token.upper() for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# remove blancs from text \n",
    "def remove_blanc(tokens):\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# expand contractions ex. this's -> this is\n",
    "def expand_contractions(sentence, contraction_mapping=CONTRACTION_MAP): \n",
    "     \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),  \n",
    "                                      flags=re.IGNORECASE|re.DOTALL) \n",
    "    def expand_match(contraction): \n",
    "        match = contraction.group(0) \n",
    "        first_char = match[0] \n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                        \n",
    "        expanded_contraction = first_char+expanded_contraction[1:] \n",
    "        return expanded_contraction \n",
    "         \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \n",
    "    return expanded_sentence \n",
    "\n",
    "# convert the text into unicode\n",
    "def remove_accent(tokens):\n",
    "    tokens = [unidecode.unidecode(token) for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# remove the stopwords from the tokenized text \n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(wordlist, stopwords=stopword_list):\n",
    "    return [w for w in wordlist if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions that handle the spelling mistakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Probability of `word`.\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    return WORDS[word] / N\n",
    "\n",
    "# Most probable spelling correction for word relative to the corpus in WORDS.\n",
    "def correct(word): \n",
    "    if len(word) > 1:\n",
    "        return max(candidates(word), key=P)\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# if there is another word in the documents similar to the input world with a relativily high\n",
    "# occurance in the documnets return the it\n",
    "# else return the input word \n",
    "def properify(word): \n",
    "    mostProbable = max(candidates_weird(word), key=P)\n",
    "    if(known([word]) and P(mostProbable)-P(word) > (P(word)/2) and len(word) > 2):\n",
    "        return mostProbable\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Generate possible spelling corrections for word\n",
    "def candidates(word): \n",
    "    return  known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "# Generate possible spelling similar to the word\n",
    "def candidates_weird(word): \n",
    "    return known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "# The subset of `words` that appear in the dictionary of WORDS\n",
    "def known(words): \n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "# All edits that are one edit away from `word`.\n",
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# All edits that are two edits away from `word`.\n",
    "def edits2(word): \n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "# impractical dont use it\n",
    "# All edits that are three edits away from `word`.\n",
    "def edits3(word): \n",
    "    return (e3 for e1 in edits1(word) for e3 in edits2(e1))\n",
    "\n",
    "# take a list of tokens and call correct on each token\n",
    "def correct_from_tokens(tokens):\n",
    "    WORDS = Counter(words(' '.join(word_list)))\n",
    "    return [correct(w) for w in tokens]\n",
    "    \n",
    "# take a list of tokens and call properify on each token\n",
    "def remove_weird_from_tokens(tokens):\n",
    "    WORDS = Counter(words(allDocuments))\n",
    "    return [properify(w) for w in tokens]\n",
    "    \n",
    "# ** not used **\n",
    "def text_blob_clean(tokens):\n",
    "    cleanBlob = TextBlob(' '.join(tokens))\n",
    "    return cleanBlob.correct()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the trained module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maher/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the classifier ...\n",
      "Loaded model from disk\n",
      "compiling the classifier\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# the location of the classifier\n",
    "path_to_classifier = '/home/maher/keywordExtraction/keywords-extraction-Project/RNN/savedModel_Python'\n",
    "\n",
    "# predict method take an input the document tokenized and output a list of predictions the \n",
    "# rest of the parameters are self explanatory\n",
    "def predict(inWords, bag_of_words, most_common, classifier, label_encoder, onehot_encoder, accuracy):\n",
    "\n",
    "    inWords = [w for w in inWords if w in bag_of_words]\n",
    "\n",
    "    test_integer_encoded = label_encoder.transform(inWords)\n",
    "    test_integer_encoded = test_integer_encoded.reshape(len(test_integer_encoded), 1)\n",
    "    X = onehot_encoder.transform(test_integer_encoded)\n",
    "    X = np.array([[w] for w in X])\n",
    "\n",
    "    pred = classifier.predict(X)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    simi_out = [inWords[i] for i in range(len(inWords)) if pred[i] > accuracy]\n",
    "    stemmed_out = []\n",
    "    out = []\n",
    "    for word in simi_out:\n",
    "        if stemmer.stem(word) not in stemmed_out:\n",
    "            out.append(word)\n",
    "            stemmed_out.append(stemmer.stem(word))\n",
    "\n",
    "    return np.unique(out)\n",
    "\n",
    "\n",
    "\n",
    "print('loading the classifier ...')\n",
    "# load json and create model\n",
    "# TODO: change the path \n",
    "json_file = open(path_to_classifier+'/classifier.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(path_to_classifier + \"/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "print('compiling the classifier')\n",
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer= 'RMSprop',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# loading LabelEncoder, OneHot Encoder, stopwords and the bag of words used \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load(path_to_classifier + '/label_encoder.npy')\n",
    "\n",
    "with open(path_to_classifier + \"/onehot_encoder.txt\", \"rb\") as fp:   # Unpickling\n",
    "    onehot_encoder = pickle.load(fp)\n",
    "with open(path_to_classifier + \"/common_words.txt\", \"rb\") as fp:   # Unpickling\n",
    "    common_words = pickle.load(fp)\n",
    "with open(path_to_classifier + \"/bag_of_words.txt\", \"rb\") as fp:   # Unpickling\n",
    "    bag_of_words = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# method getTags take as an input the document tokenized and the accuracy needed and return the predictions  \n",
    "def getTags(tokens, accuracy):\n",
    "    tags = predict(tokens, bag_of_words, common_words, loaded_model, label_encoder, onehot_encoder, accuracy)\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tags of a specific document, by its number\n",
    "def getTagsByDocNum(documentNumber):\n",
    "\n",
    "    inputDoc = docs['content'][documentNumber]\n",
    "    inputDocList = regexp_tokenize(expand_contractions(inputDoc), pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "    no_weird_words = remove_weird_from_tokens(remove_blanc(convert_letters(inputDocList)))\n",
    "\n",
    "    clean = correct_from_tokens(no_weird_words)\n",
    "\n",
    "    tags = getTags(clean, 0.7)\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Tags of All the documnets in a list, each document separated by '###'\n",
    "def getAllTags():\n",
    "#     currently the setting the number of iterations to 2 as the lack of computational resources\n",
    "    numberOfDocuments = 2 \n",
    "    allPredictions = ''\n",
    "    print(len(docs))\n",
    "    for documentNumber in range(numberOfDocuments):\n",
    "        if(not isinstance(docs['content'][documentNumber], type(0.0))):\n",
    "            inputDoc = docs['content'][documentNumber]\n",
    "            inputDocList = regexp_tokenize(expand_contractions(inputDoc), pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "            no_weird_words = remove_weird_from_tokens(remove_blanc(convert_letters(inputDocList)))\n",
    "\n",
    "            clean = correct_from_tokens(no_weird_words)\n",
    "\n",
    "            tags = getTags(clean, 0.7)\n",
    "\n",
    "            allPredictions = allPredictions + ' '.join(tags)\n",
    "            allPredictions = allPredictions + ' ### '\n",
    "\n",
    "            print(len(allPredictions))\n",
    "            print(documentNumber)\n",
    "    return allPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5d693f2d3584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetAllTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-5c66c2b42c04>\u001b[0m in \u001b[0;36mgetAllTags\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minputDocList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\w+|\\$[\\d\\.]+|\\S+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mno_weird_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_weird_from_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_blanc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_letters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDocList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_from_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_weird_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36mremove_weird_from_tokens\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_weird_from_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallDocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproperify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# ** not used **\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_weird_from_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallDocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproperify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# ** not used **\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36mproperify\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# else return the input word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mproperify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmostProbable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates_weird\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmostProbable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmostProbable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36mcandidates_weird\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Generate possible spelling similar to the word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcandidates_weird\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# The subset of `words` that appear in the dictionary of WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36mknown\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# The subset of `words` that appear in the dictionary of WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# All edits that are one edit away from `word`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# The subset of `words` that appear in the dictionary of WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# All edits that are one edit away from `word`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# All edits that are two edits away from `word`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0medits2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# impractical dont use it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36medits1\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdeletes\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtransposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mreplaces\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0minserts\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeletes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtransposes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreplaces\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minserts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf47a72f545a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdeletes\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtransposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mreplaces\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0minserts\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeletes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtransposes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreplaces\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minserts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "getAllTags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cards', 'curve', 'random', 'resource', 'sheet', 'team', 'word'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTagsByDocNum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `getAllTags()` method on a cloud server and modefying it to get only 100 documnets, I got the following set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions100 = '''builder cards children curve frequency guided inventory mm random resource ride sheet state straight team word ### cards children curve frequency guided inventory mate music random resource sheet word ### builder cards children curve frequency guided inventory motor random resource sheet word ### cards children color cut frequency guided inventory mate notch random resource sheet word ### builder cards children curve frequency guided inventory random resource school sheet straight word ### arms builder cards children curve frequency guided inventory random resource sheet strip word ### builder cards children curve frequency guided house inventory music random resource sheet word ### arc builder cards children frame group inventory power repeat resource sentence sheet stories straight word ### builder cards children feedback frequency guided random repeat resource sentence sprung stories straight word ### builder cards children feedback frequency guided inventory random repeat resource stories word ### builder cards children curve feedback frequency game guided inventory prostate random repeat resource sentence stories straight word ### builder cards children curve feedback frequency gray guided mate push random repeat resource sentence stories straight word ### builder cards children desk feedback frequency guided random repeat resource sentence stories straight word ### builder cards children feedback frequency guided inventory random repeat resource sentence stories word ### builder cards children curve feedback frequency guided heart house inventory mate random repeat resource rough sentence stories straight word ### builder capital cards children feedback finger frequency guided house inventory random repeat resource sentence stories word ### builder cards children curve edge feedback frequency random repeat resource sentence stories straight word ### builder cards children curve feedback frequency guided inventory random repeat resource sentence straight tract word ### builder cards children curve feedback frequency guided inventory mate powder random repeat resource stories straight word zebra ### builder cards children curve feedback frequency guided inventory random repeat resource sentence stories straight word ### @ group management resource word ### @ group management resource word ### @ group management resource word ### labels resource word ### labels resource word ### labels resource word ### children media random repeat ride word ### builder cards children error graph learning repeat word ### builder cards children error graph learning repeat word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### arc children frequency group house job repeat sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### children sentence trees word ### banks cards children feedback frame modeling professional sentence speech word ### banks cards children feedback frame modeling professional sentence speech word ### banks cards children feedback frame modeling professional sentence speech word ### children knowledge stories tree word ### children desk distance guided music rules selection sentence web word ### children desk distance guided music rules selection sentence web word ### children group guided language mm path repeat sentence transparency word ### capital cards children classroom guided labels rates self sentence transparency word ### builder cards children frequency repeat word ### builder cards children frequency repeat word ### builder cards children frequency repeat word ### children group mate resource sentence ### children group mate resource sentence ### children group mate resource sentence ### children group mate resource sentence ### children expert guided information personal riding ### children direct information personal word ### children direct information personal word ### children frequency group mark repeat word ### children frequency group mark repeat word ### @ answering children information knowledge learning mark mate memories mm repeat selection sentence social stories transparency word ### @ answering children information knowledge learning mark mate memories mm repeat selection sentence social stories transparency word ### children frame repeat team word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### children distance group sheet word ### children distance group sheet word ### children group guided language sentence ### cards children classroom cut health labels resource school sheet word ### cards children classroom cut health labels resource school sheet word ### builder cards children dual frame patterns repeat sentence transparency word ### builder cards children dual frame patterns repeat sentence transparency word ### children coding frequency spacing torso white word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### bus children paired personal repeat selection ### bus children paired personal repeat selection ### cards children connections phone word ### cards children connections phone word ### cards children connections phone word ### '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see the occurence of each prediction, and exclude the repetitive ones as they are common in all the documents and don't define this particlar documnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 294), ('children', 92), ('word', 86), ('cards', 50), ('repeat', 47), ('sentence', 46), ('guided', 42), ('resource', 42), ('frequency', 39), ('group', 34), ('builder', 25), ('random', 24), ('inventory', 16), ('stories', 15), ('feedback', 15), ('transparency', 15), ('curve', 13), ('sheet', 12), ('straight', 12), ('speech', 12)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predTokens = nltk.tokenize.word_tokenize(predictions100)\n",
    "fdist  = FreqDist(predTokens)\n",
    "most30 = fdist.most_common(20)\n",
    "print(most30)\n",
    "most30 = [w for (w,x) in most30]\n",
    "most30.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mm', 'ride', 'state', 'team', '#']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# excluding the repetitive ones from the predictions\n",
    "cleanedPredictions100 = [w for w in predTokens if w not in most30]\n",
    "cleanedPredictions100[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
