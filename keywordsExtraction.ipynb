{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import nltk.data\n",
    "import pattern3\n",
    "import re\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import brown\n",
    "word_list = brown.words()\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and visualize the data to gain insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv('documents-fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4087, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lesson 1\\nConsonant: /m/m\\nH1gh-Frequency Words: 1, a\\nObject1ves\\n• ro match the Sound \\'m/ to the\\n1etter m\\n• ro wr1\\'e uppercase and 1owercase\\n• To read h1gh-frequency words 1\\nand a\\nMater1a1s\\n• Sound/SpdJ1ng Cam Mm\\n• 8\\'Nder Cords\\nCams: 1, my, 11ke, the, he, to\\n• word cards1 und for each ch11d\\nreacher Resource BOOK, p. P113\\n• Pract1ce Book, page 11\\nG rode\\nConsonant Mm\\n1NTRODUCE Mm D1sp1ay Sound/SpeWng card\\nMm. Te11 ch11dren that the nome Of th1s 1etter 1s M.\\nTouch the card. say M, Ond have ch11dren say the\\n1etter name. Touch the card severa1 t1mes and have\\nCh11dren say M each t1me.\\nWR1TE Mm Mode1 how to wr1te uppercase M and\\n1owercase m on the board as fo11ows:\\n1ETTER TA1K FORM Stra1ght down, s1ant r1ght,\\nS1ant up, down.\\n1ETTER TA1K FOR m up and c u Ne down, \\'Spe111ng\\nCard \"m\\nup and curve down.\\nPRACT1CE Have Ch11dren proct1ce wr1t1ng uppercase M and\\n1owercase m on a sheet or paper. Ask them to c1rc1e the1r best Mm.\\nRe1ate \\'m/ to m\\nMATCH SOUND TO 1ETTER Say mat s1ow1y. emphas1z1ng and\\nhove the ch11dren do the same. Te11 them that the 1etter m stands tor\\nthe 1m/ sound. Po1nt to Sound/SpeH1ng Cord Mm and hove ch11dren\\nsay {m/. Touch the card severa1 t1mes and hove them soy /rn! each\\nt1me.\\nPRACT1CE D1str1bute Word Bu11der Card m. Te11 ch11dren: W111 say\\nsome worts. 1f the word beg1ns w1th the fm/ sound. ho1d up your m\\ncard. 1f 1t doesn\\'t beg1n w1th 1m/ , do not ho1d up your\\nSoy the words map, rnjce, dog, book, runny, and mother, hav1ng\\nch11dren ho1d up the1r m card for each word thot beg1ns \"1th the 1m/\\nsound. Then te11 ch11drenz w111 say same 1f the word ends\\nW1th the tm/ sound, ho1d up your m card. 1f 1t doesn\\'t end W1th fm\\'.\\ndo not ho1d up your card.\\nSay the words hum, skate, c1am, room, jar, and team, hov1ng ch11dren\\nho1d up the1r m card tor each word that ends W1th the sound.\\n.0\\nH1gh-frequency Words: 1. a\\n1NTRODUCEJ wr1te the ward 1on board.\\n• Po1nt to Ond read r. Have ch11dren read 1t W1th you\\n• Say: 1con run fast.\\n• D1sp1ay the Word Card 1. Say: r.\\n• Match the Word Card to the word an the board, soy the word,\\nand have ch11dren say 1t W1th you.\\nwr1te wordaon the board.\\n• Po1nt to and read a. Hove ch11dren read 1t w1th you\\nSoy: We hove a cot.\\n• D1sp1ay the Word Cord d. Say: a.\\nMatch the Word Card to the word on the board, Soy the word,\\nand have ch11dren Soy 1t w1th you.\\nGU1DED PRACT1CE Dup11cate word cords 1 and a (Teacher Resource\\nBook. p. 243) for each ch11d. D1str1bute the word cards for Have\\nch11dren po1nt to the word and read 1t. Te11 ch11dren that you w111 Show\\nthem words, and when they seer, they shou1d SOY the word and\\npo1nt ta 1t on the1r curd. Ho1d up the fo11ow1ng Word Cards 1n random\\norder unt11 ch11dren cons1stent1y 1dent1ty the word 1.\\n11ke\\nthe\\nD1str1bute the word cords for a. Hove Ch11dren po1nt to the word\\nand read 1t. Ho1d up the fo11ow1ng Word Cards 1n random order unt11\\nch11dren cons1stent1y 1dent1fy the word a.\\nhe\\nto\\nA Book. p. 11\\nlesson • 1nventory Un1t\\n13'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[\"core_std\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7599104\n"
     ]
    }
   ],
   "source": [
    "# Concatinate all words from all the documents.\n",
    "allDocuments = ''\n",
    "for i in range(len(docs)):\n",
    "    if(not isinstance(docs['content'][i], type(0.0))):\n",
    "        allDocuments = allDocuments + str(docs['content'][i])\n",
    "print(len(allDocuments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(['m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating pipeline functions for handling the data (content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions that handle the text format and style of writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the tokens to lowercase/uppercase\n",
    "def convert_letters(tokens, style = \"lower\"):\n",
    "    if (style == \"lower\"):\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    else :\n",
    "        tokens = [token.upper() for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# remove blancs from text \n",
    "def remove_blanc(tokens):\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# expand contractions ex. this's -> this is\n",
    "def expand_contractions(sentence, contraction_mapping=CONTRACTION_MAP): \n",
    "     \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),  \n",
    "                                      flags=re.IGNORECASE|re.DOTALL) \n",
    "    def expand_match(contraction): \n",
    "        match = contraction.group(0) \n",
    "        first_char = match[0] \n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                        \n",
    "        expanded_contraction = first_char+expanded_contraction[1:] \n",
    "        return expanded_contraction \n",
    "         \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \n",
    "    return expanded_sentence \n",
    "\n",
    "# convert the text into unicode\n",
    "def remove_accent(tokens):\n",
    "    tokens = [unidecode.unidecode(token) for token in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# remove the stopwords from the tokenized text \n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(wordlist, stopwords=stopword_list):\n",
    "    return [w for w in wordlist if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions that handle the spelling mistakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Probability of `word`.\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    global WORDS\n",
    "    return WORDS[word] / N\n",
    "\n",
    "# Most probable spelling correction for word relative to the corpus in WORDS.\n",
    "def correct(word): \n",
    "    if len(word) > 1:\n",
    "        return max(candidates(word), key=P)\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# if there is another word in the documents similar to the input world with a relativily high\n",
    "# occurance in the documnets return the it\n",
    "# else return the input word \n",
    "def properify(word): \n",
    "    mostProbable = max(candidates_weird(word), key=P)\n",
    "    if(known([word]) and P(mostProbable)-P(word) > (P(word)/2) and len(word) > 2):\n",
    "        return mostProbable\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Generate possible spelling corrections for word\n",
    "def candidates(word): \n",
    "    return  known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "# Generate possible spelling similar to the word\n",
    "def candidates_weird(word): \n",
    "    return known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "# The subset of `words` that appear in the dictionary of WORDS\n",
    "def known(words): \n",
    "    global WORDS\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "# All edits that are one edit away from `word`.\n",
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# All edits that are two edits away from `word`.\n",
    "def edits2(word): \n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "# impractical dont use it\n",
    "# All edits that are three edits away from `word`.\n",
    "def edits3(word): \n",
    "    return (e3 for e1 in edits1(word) for e3 in edits2(e1))\n",
    "\n",
    "# take a list of tokens and call correct on each token\n",
    "def correct_from_tokens(tokens):\n",
    "    global WORDS\n",
    "    WORDS = Counter(words(' '.join(word_list)))\n",
    "    return [correct(w) for w in tokens]\n",
    "    \n",
    "# take a list of tokens and call properify on each token\n",
    "def remove_weird_from_tokens(tokens):\n",
    "    global WORDS\n",
    "    WORDS = Counter(words(allDocuments))\n",
    "    return [properify(w) for w in tokens]\n",
    "    \n",
    "# ** not used **\n",
    "def text_blob_clean(tokens):\n",
    "    cleanBlob = TextBlob(' '.join(tokens))\n",
    "    return cleanBlob.correct()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the trained module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the classifier ...\n",
      "Loaded model from disk\n",
      "compiling the classifier\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# the location of the classifier\n",
    "path_to_classifier = '/home/maher/Desktop/optimaKeywordsTask/savedModel'\n",
    "\n",
    "# predict method take an input the document tokenized and output a list of predictions the \n",
    "# rest of the parameters are self explanatory\n",
    "def predict(inWords, bag_of_words, most_common, classifier, label_encoder, onehot_encoder, accuracy):\n",
    "\n",
    "    inWords = [w for w in inWords if w in bag_of_words]\n",
    "\n",
    "    test_integer_encoded = label_encoder.transform(inWords)\n",
    "    test_integer_encoded = test_integer_encoded.reshape(len(test_integer_encoded), 1)\n",
    "    X = onehot_encoder.transform(test_integer_encoded)\n",
    "    X = np.array([[w] for w in X])\n",
    "\n",
    "    pred = classifier.predict(X)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    simi_out = [inWords[i] for i in range(len(inWords)) if pred[i] > accuracy]\n",
    "    stemmed_out = []\n",
    "    out = []\n",
    "    for word in simi_out:\n",
    "        if stemmer.stem(word) not in stemmed_out:\n",
    "            out.append(word)\n",
    "            stemmed_out.append(stemmer.stem(word))\n",
    "\n",
    "    return np.unique(out)\n",
    "\n",
    "\n",
    "\n",
    "print('loading the classifier ...')\n",
    "# load json and create model\n",
    "# TODO: change the path \n",
    "json_file = open(path_to_classifier+'/classifier.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(path_to_classifier + \"/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "print('compiling the classifier')\n",
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer= 'RMSprop',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# loading LabelEncoder, OneHot Encoder, stopwords and the bag of words used \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load(path_to_classifier + '/label_encoder.npy')\n",
    "\n",
    "with open(path_to_classifier + \"/onehot_encoder.txt\", \"rb\") as fp:   # Unpickling\n",
    "    onehot_encoder = pickle.load(fp)\n",
    "with open(path_to_classifier + \"/common_words.txt\", \"rb\") as fp:   # Unpickling\n",
    "    common_words = pickle.load(fp)\n",
    "with open(path_to_classifier + \"/bag_of_words.txt\", \"rb\") as fp:   # Unpickling\n",
    "    bag_of_words = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# method getTags take as an input the document tokenized and the accuracy needed and return the predictions  \n",
    "def getTags(tokens, accuracy):\n",
    "    tags = predict(tokens, bag_of_words, common_words, loaded_model, label_encoder, onehot_encoder, accuracy)\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tags of a specific document, by its number\n",
    "def getTagsByDocNum(documentNumber):\n",
    "    global WORDS\n",
    "    inputDoc = docs['content'][documentNumber]\n",
    "    inputDocList = regexp_tokenize(expand_contractions(inputDoc), pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "    no_weird_words = remove_weird_from_tokens(remove_blanc(convert_letters(inputDocList)))\n",
    "\n",
    "    clean = correct_from_tokens(no_weird_words)\n",
    "\n",
    "    tags = getTags(clean, 0.7)\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Tags of All the documnets in a list, each document separated by '###'\n",
    "def getAllTags():\n",
    "#     currently the setting the number of iterations to 2 as the lack of computational resources\n",
    "    global WORDS\n",
    "    numberOfDocuments = 2 \n",
    "    allPredictions = ''\n",
    "    print(len(docs))\n",
    "    for documentNumber in range(numberOfDocuments):\n",
    "        if(not isinstance(docs['content'][documentNumber], type(0.0))):\n",
    "            inputDoc = docs['content'][documentNumber]\n",
    "            inputDocList = regexp_tokenize(expand_contractions(inputDoc), pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "            no_weird_words = remove_weird_from_tokens(remove_blanc(convert_letters(inputDocList)))\n",
    "\n",
    "            clean = correct_from_tokens(no_weird_words)\n",
    "\n",
    "            tags = getTags(clean, 0.7)\n",
    "\n",
    "            allPredictions = allPredictions + ' '.join(tags)\n",
    "            allPredictions = allPredictions + ' ### '\n",
    "\n",
    "            print(len(allPredictions))\n",
    "            print(documentNumber)\n",
    "    return allPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4087\n",
      "115\n",
      "0\n",
      "205\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'builder cards children curve frequency guided inventory mm random resource ride sheet state straight team word ### cards children curve frequency guided inventory mate music random resource sheet word ### '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAllTags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['builder', 'cards', 'children', 'curve', 'frequency', 'guided',\n",
       "       'inventory', 'mm', 'random', 'resource', 'ride', 'sheet', 'state',\n",
       "       'straight', 'team', 'word'], dtype='<U9')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTagsByDocNum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `getAllTags()` method on a cloud server and modefying it to get only 100 documnets, I got the following set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions100 = '''builder cards children curve frequency guided inventory mm random resource ride sheet state straight team word ### cards children curve frequency guided inventory mate music random resource sheet word ### builder cards children curve frequency guided inventory motor random resource sheet word ### cards children color cut frequency guided inventory mate notch random resource sheet word ### builder cards children curve frequency guided inventory random resource school sheet straight word ### arms builder cards children curve frequency guided inventory random resource sheet strip word ### builder cards children curve frequency guided house inventory music random resource sheet word ### arc builder cards children frame group inventory power repeat resource sentence sheet stories straight word ### builder cards children feedback frequency guided random repeat resource sentence sprung stories straight word ### builder cards children feedback frequency guided inventory random repeat resource stories word ### builder cards children curve feedback frequency game guided inventory prostate random repeat resource sentence stories straight word ### builder cards children curve feedback frequency gray guided mate push random repeat resource sentence stories straight word ### builder cards children desk feedback frequency guided random repeat resource sentence stories straight word ### builder cards children feedback frequency guided inventory random repeat resource sentence stories word ### builder cards children curve feedback frequency guided heart house inventory mate random repeat resource rough sentence stories straight word ### builder capital cards children feedback finger frequency guided house inventory random repeat resource sentence stories word ### builder cards children curve edge feedback frequency random repeat resource sentence stories straight word ### builder cards children curve feedback frequency guided inventory random repeat resource sentence straight tract word ### builder cards children curve feedback frequency guided inventory mate powder random repeat resource stories straight word zebra ### builder cards children curve feedback frequency guided inventory random repeat resource sentence stories straight word ### @ group management resource word ### @ group management resource word ### @ group management resource word ### labels resource word ### labels resource word ### labels resource word ### children media random repeat ride word ### builder cards children error graph learning repeat word ### builder cards children error graph learning repeat word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### cards children group professional word ### arc children frequency group house job repeat sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### @ children frame frequency house knowledge nest notch repeat ride sentence word ### children sentence trees word ### banks cards children feedback frame modeling professional sentence speech word ### banks cards children feedback frame modeling professional sentence speech word ### banks cards children feedback frame modeling professional sentence speech word ### children knowledge stories tree word ### children desk distance guided music rules selection sentence web word ### children desk distance guided music rules selection sentence web word ### children group guided language mm path repeat sentence transparency word ### capital cards children classroom guided labels rates self sentence transparency word ### builder cards children frequency repeat word ### builder cards children frequency repeat word ### builder cards children frequency repeat word ### children group mate resource sentence ### children group mate resource sentence ### children group mate resource sentence ### children group mate resource sentence ### children expert guided information personal riding ### children direct information personal word ### children direct information personal word ### children frequency group mark repeat word ### children frequency group mark repeat word ### @ answering children information knowledge learning mark mate memories mm repeat selection sentence social stories transparency word ### @ answering children information knowledge learning mark mate memories mm repeat selection sentence social stories transparency word ### children frame repeat team word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### cards children selection speech state transparency word ### children distance group sheet word ### children distance group sheet word ### children group guided language sentence ### cards children classroom cut health labels resource school sheet word ### cards children classroom cut health labels resource school sheet word ### builder cards children dual frame patterns repeat sentence transparency word ### builder cards children dual frame patterns repeat sentence transparency word ### children coding frequency spacing torso white word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### bank children frequency group guided repeat resource sentence word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### cards children group guided head mark random repeat sentence speech word ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### children guided knowledge logging rates transparency ### bus children paired personal repeat selection ### bus children paired personal repeat selection ### cards children connections phone word ### cards children connections phone word ### cards children connections phone word ### '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see the occurence of each prediction, and exclude the repetitive ones as they are common in all the documents and don't define this particlar documnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 294), ('children', 92), ('word', 86), ('cards', 50), ('repeat', 47), ('sentence', 46), ('guided', 42), ('resource', 42), ('frequency', 39), ('group', 34), ('builder', 25), ('random', 24), ('inventory', 16), ('stories', 15), ('feedback', 15), ('transparency', 15), ('curve', 13), ('sheet', 12), ('straight', 12), ('speech', 12)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predTokens = nltk.tokenize.word_tokenize(predictions100)\n",
    "fdist  = FreqDist(predTokens)\n",
    "most30 = fdist.most_common(20)\n",
    "print(most30)\n",
    "most30 = [w for (w,x) in most30]\n",
    "most30.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mm', 'ride', 'state', 'team', '#']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# excluding the repetitive ones from the predictions\n",
    "cleanedPredictions100 = [w for w in predTokens if w not in most30]\n",
    "cleanedPredictions100[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
